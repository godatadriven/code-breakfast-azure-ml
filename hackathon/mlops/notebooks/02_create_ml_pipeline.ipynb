{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an ML pipeline\n",
    "\n",
    "To take our first steps towards an MLOps approach for training and deploying our model, we need to create a [Machine Learning pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines) in Azure ML. \n",
    "\n",
    "In essence, a Machine Learning Pipeline is an independently executable workflow of a complete machine learning task. Each subtask (e.g., training the model, evaluating the trained model, etc.) are implemented as a step (or series of steps) within the pipeline.\n",
    "\n",
    "In principle, you can do whatever you want within a pipeline, as pipeline steps can also include generic tasks such as running Python scripts. However, we would recommend that pipelines focus on specific machine learning tasks such as:\n",
    "\n",
    "* Data preparation (validating, cleaning, munging data, etc.)\n",
    "* Training and validating machine learning model(s)\n",
    "* Deploying trained machine learning models\n",
    "\n",
    "## Getting started\n",
    "\n",
    "In this exercise, we would like to build a pipeline that includes the following steps:\n",
    "\n",
    "* Preprocess (which does some simple preprocessing on the dataset)\n",
    "* Train (which takes training data and produces a model)\n",
    "* Evaluate (which logs metrics/parameters for the trained model)\n",
    "* Register (which registers the trained model in Azure ML)\n",
    "\n",
    "For each of these steps, we also provide an script (entrypoint) in the model. You can find these scripts in the model/scripts directory. Note that you can also run the scripts locally if you want to try them out!\n",
    "\n",
    "For example, you can train the model locally using:\n",
    "\n",
    "   `python model/scripts/train.py --input_dir ../../datasets/titanic --input_file train.csv --model_dir outputs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the initial pipeline\n",
    "\n",
    "To build our ML pipeline, we have to do the following:\n",
    "\n",
    "* Specify the steps to include in the pipeline\n",
    "* Define dependencies between steps using inputs/outputs\n",
    "\n",
    "Besides this, we need to configure each step to tell Azure ML what to run and where. For a PythonScriptStep, this includes the following configuration options:\n",
    "\n",
    "* RunConfig (= environment to run in terms of Python dependencies, etc.)\n",
    "* Source + entrypoints (= code to run in the step)\n",
    "* Compute target (= compute resource to run on, e.g. VM cluster, databricks, etc.)\n",
    "* Inputs/outputs (= datasets/paths to use for inputs/outputs)\n",
    "\n",
    "### Setting up a run config\n",
    "\n",
    "Let's start by getting our ML workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the workspace, we can setting up the config for our steps by defining our RunConfig. This RunConfig essentially defines the runtime environment in which our steps will be run. In principle, you can define a different RunConfig for each step, but we'll stick to using one common environment for now.\n",
    "\n",
    "To keep our RunConfig reproducible, we'll base it off our (model) environment.yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from azureml.core import RunConfiguration, Environment\n",
    "\n",
    "model_dir = Path(\"../model\")\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = Environment.from_conda_specification(\n",
    "    \"model-env\", model_dir / \"environment.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a compute target\n",
    "\n",
    "Next, we'll set up our compute target using a helper function, which we'll similarly share across steps. This helper function essentially creates a cluster of VMs in Azure ML, which uses a specific type of VM and supports auto-scaling between a min/max number of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "def _get_or_create_cluster(\n",
    "    workspace,\n",
    "    name,\n",
    "    vm_size=\"STANDARD_D2_V2\",\n",
    "    vm_priority=\"lowpriority\",\n",
    "    min_nodes=0,\n",
    "    max_nodes=4,\n",
    "    idle_seconds_before_scaledown=\"300\",\n",
    "    wait=False,\n",
    "):\n",
    "    \"\"\"Helper function for creating a cluster of VMs as compute target.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # pylint: disable=abstract-class-instantiated\n",
    "        target = ComputeTarget(workspace=workspace, name=name)\n",
    "        print(\"Using existing cluster %s\" % name)\n",
    "    except ComputeTargetException: \n",
    "        print(\"Creating cluster %s\" % name)\n",
    "        config = AmlCompute.provisioning_configuration(\n",
    "            vm_size=vm_size,\n",
    "            vm_priority=vm_priority,\n",
    "            min_nodes=min_nodes,\n",
    "            max_nodes=max_nodes,\n",
    "            idle_seconds_before_scaledown=idle_seconds_before_scaledown,\n",
    "        )\n",
    "        target = ComputeTarget.create(workspace, name, config)\n",
    "\n",
    "        if wait:\n",
    "            target.wait_for_completion(show_output=False)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "compute_target = _get_or_create_cluster(workspace, name=\"my-cluster\", wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining data references\n",
    "\n",
    "Now we have our basic config set up, we also need to define where our data comes from. In this case, we'll use an instance of the [DataReference](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.data_reference.datareference?view=azure-ml-py) class to reference the data that we uploaded to our datastore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "\n",
    "train_data = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"train_data\",\n",
    "    path_on_datastore=\"titanic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reference will allow our pipeline steps to reference the dataset on the datastore, allowing Azure ML to fetch the dataset before running the step.\n",
    "\n",
    "Besides this, we also need to have a place to store intermediate output data from our pipeline. This includes, for example, the preprocessed version of the train dataset produced by the *preprocess* step, as well as the model pickle produced by the *train* step.\n",
    "\n",
    "We can create intermediate storage for our pipeline using the the [PipelineData](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "preprocessed_data = PipelineData(\n",
    "    \"preprocessed_data\", \n",
    "    datastore=datastore,\n",
    ")\n",
    "\n",
    "model_data = PipelineData(\n",
    "    \"model_data\", \n",
    "    datastore=datastore, \n",
    "    pipeline_output_name=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pipeline + steps\n",
    "\n",
    "Now, to finally start building our actual pipeline, we can define our first *preprocess* step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "preprocess_step = PythonScriptStep(\n",
    "    name=\"preprocess\",\n",
    "    source_directory=str(model_dir),\n",
    "    script_name=\"scripts/preprocess.py\",\n",
    "    arguments=[\"--input_dir\", train_data, \"--output_dir\", preprocessed_data],\n",
    "    inputs=[train_data],\n",
    "    outputs=[preprocessed_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this PythonScriptStep definition does the following:\n",
    "* We call our step *'preprocess'*\n",
    "* We tell the step to use the code in our model directory and call the *preprocess.py* script.\n",
    "* We define the arguments to pass to the script, which reference our pipeline storage.\n",
    "* We define the inputs/outputs, which point to our pipeline storage (note: this is used to define dependencies between tasks).\n",
    "* We also define our compute target + run config for the step.\n",
    "\n",
    "Now that we have our first step, we can create an initial version of the pipeline as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    workspace=workspace,\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "    ],\n",
    ")\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the *validate* method checks if the pipeline *looks* valid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the pipeline\n",
    "\n",
    "To run our pipeline, we first need to publish it to Azure ML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(\n",
    "    name=\"my-first-pipeline\",\n",
    "    description=\"Description of my first pipeline.\",\n",
    "    continue_on_step_failure=False,\n",
    ") \n",
    "\n",
    "print(published_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once published, you should be able to see the pipeline in your Azure ML workspace (under Pipelines > Endpoints). (Check if you can find your pipeline!) \n",
    "\n",
    "You can retrieve your pipeline using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can fetch pipeline using the pipeline ID as follows:\n",
    "# from azureml.pipeline.core import PipelineRun, PublishedPipeline\n",
    "# pipeline = PublishedPipeline.get(workspace=workspace, id=pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can trigger our pipeline by sending a POST request to the pipeline's HTTP endpoint. Note that we also need to define an experiment to run our pipeline in, as the experiment will be responsible for tracking model parameters etc.\n",
    "\n",
    "To trigger our pipeline, you can perform the required request as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "\n",
    "def _wait_for_run_completion(pipeline_run):\n",
    "    \"\"\"Helper function that waits for the pipeline to finish.\"\"\"\n",
    "    \n",
    "    JOB_STATUS = {\n",
    "        \"not_started\": {0, \"NotStarted\"},\n",
    "        \"running\": {1, \"Running\"},\n",
    "        \"failed\": {2, \"Failed\"},\n",
    "        \"cancelled\": {3, \"Cancelled\"},\n",
    "        \"finished\": {4, \"Finished\"},\n",
    "    }\n",
    "    \n",
    "    print(\"Waiting for job to start...\")\n",
    "    status = pipeline_run.get_status()\n",
    "    while status in JOB_STATUS[\"not_started\"]:\n",
    "        time.sleep(1)\n",
    "        status = pipeline_run.get_status()\n",
    "\n",
    "    if status in JOB_STATUS[\"running\"]:\n",
    "        print(\"Job started, waiting for completion...\")\n",
    "        while status in JOB_STATUS[\"running\"]:\n",
    "            time.sleep(1)\n",
    "            status = pipeline_run.get_status()\n",
    "\n",
    "    if status in JOB_STATUS[\"finished\"]:\n",
    "        print(\"Job finished successfully!\")\n",
    "    elif status in JOB_STATUS[\"failed\"]:\n",
    "        print(\"ERROR: Job failed!\")\n",
    "    elif status in JOB_STATUS[\"cancelled\"]:\n",
    "        print(\"WARNING: Job was cancelled.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected status '{status}'\")\n",
    "        \n",
    "    \n",
    "# Define the experiment to run in.\n",
    "experiment_name = \"my-first-experiment\"\n",
    "\n",
    "# Get the required authentication token for the endpoint.\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "# Define parameters for our request.\n",
    "request_payload = {\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"ParameterAssignments\": {},\n",
    "}\n",
    "\n",
    "# Perform request + check response.\n",
    "response = requests.post(published_pipeline.endpoint, headers=aad_token, json=request_payload)\n",
    "response.raise_for_status()\n",
    "\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(\"Job ID: %s\" % run_id)\n",
    " \n",
    "# Retrieve the corresponding pipeline run and wait for it to finish.\n",
    "pipeline_run = PipelineRun.get(workspace, run_id)\n",
    "_wait_for_run_completion(pipeline_run) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this should trigger a pipeline run and wait for it to finish. Note that you should be able to monitor the progress from the Azure ML interface as well (under Experiments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the rest of the pipeline\n",
    "\n",
    "Now we have a start, we want to start building the rest of our pipeline. The idea is to build a pipeline that includes the following steps:\n",
    "\n",
    "* Preprocessing (which we already implemented).\n",
    "* Train - for training the model.\n",
    "* Evaluate - which logs metrics/parameters for the model.\n",
    "* Register - which registers the model in Azure ML.\n",
    "\n",
    "Altogether, the pipeline should look something like this:\n",
    "\n",
    "<img src=\"images/pipeline.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Implement the above pipeline by adding extra PythonScriptSteps that implement each of the described steps.\n",
    "* Try publishing the new pipeline and running the pipeline to train and register a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment: See if you can find the running pipeline in your Azure ML portal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra exercises\n",
    "\n",
    "- Try building the above example using the Dataset API (instead of using the *DataReference* class).\n",
    "- Try using other, more specific pipeline steps (such as the sklearn or Estimator related steps).\n",
    "- Try adding AutoML or hyper parameter optimization to the pipeline (using the HyperDriveStep)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
