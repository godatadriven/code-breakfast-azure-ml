{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from azureml.core import RunConfiguration, Environment\n",
    "\n",
    "model_dir = Path(\"../model\")\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment = Environment.from_conda_specification(\n",
    "    \"model-env\", model_dir / \"environment.yml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "def _get_or_create_cluster(\n",
    "    workspace,\n",
    "    name,\n",
    "    vm_size=\"STANDARD_D2_V2\",\n",
    "    vm_priority=\"lowpriority\",\n",
    "    min_nodes=0,\n",
    "    max_nodes=4,\n",
    "    idle_seconds_before_scaledown=\"300\",\n",
    "    wait=False,\n",
    "):\n",
    "    \"\"\"Helper function for creating a cluster of VMs as compute target.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # pylint: disable=abstract-class-instantiated\n",
    "        target = ComputeTarget(workspace=workspace, name=name)\n",
    "        print(\"Using existing cluster %s\" % name)\n",
    "    except ComputeTargetException: \n",
    "        print(\"Creating cluster %s\" % name)\n",
    "        config = AmlCompute.provisioning_configuration(\n",
    "            vm_size=vm_size,\n",
    "            vm_priority=vm_priority,\n",
    "            min_nodes=min_nodes,\n",
    "            max_nodes=max_nodes,\n",
    "            idle_seconds_before_scaledown=idle_seconds_before_scaledown,\n",
    "        )\n",
    "        target = ComputeTarget.create(workspace, name, config)\n",
    "\n",
    "        if wait:\n",
    "            target.wait_for_completion(show_output=False)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "compute_target = _get_or_create_cluster(workspace, name=\"my-cluster\", wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "datastore = workspace.get_default_datastore()\n",
    "\n",
    "train_data = DataReference(\n",
    "    datastore=datastore,\n",
    "    data_reference_name=\"train_data\",\n",
    "    path_on_datastore=\"titanic\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "\n",
    "preprocessed_data = PipelineData(\n",
    "    \"preprocessed_data\", \n",
    "    datastore=datastore,\n",
    ")\n",
    "\n",
    "model_data = PipelineData(\n",
    "    \"model_data\", \n",
    "    datastore=datastore, \n",
    "    pipeline_output_name=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "preprocess_step = PythonScriptStep(\n",
    "    name=\"preprocess\",\n",
    "    script_name=\"scripts/preprocess.py\",\n",
    "    arguments=[\"--input_dir\", train_data, \"--output_dir\", preprocessed_data],\n",
    "    inputs=[train_data],\n",
    "    outputs=[preprocessed_data],\n",
    "    compute_target=compute_target,\n",
    "    runconfig=run_config,\n",
    "    source_directory=str(model_dir),\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step preprocess is ready to be created [aa25c266]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    workspace=workspace,\n",
    "    steps=[\n",
    "        preprocess_step, \n",
    "    ],\n",
    ")\n",
    "pipeline.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step preprocess [aa25c266][60bc56cf-b688-4500-920f-805f22e0b1b7], (This step will run and generate new outputs)\n",
      "Using data reference train_data for StepId [00af37ce][bc992f3e-4530-44e8-8a0b-28d75f6a510b], (Consumers of this data are eligible to reuse prior runs.)\n",
      "Pipeline(Name: my-first-pipeline,\n",
      "Id: 591b06a0-83c5-4a1e-9798-befc5c0197e8,\n",
      "Status: Active,\n",
      "Endpoint: https://westeurope.aether.ms/api/v1.0/subscriptions/5ddf05c0-b972-44ca-b90a-3e49b5de80dd/resourceGroups/julian-playground/providers/Microsoft.MachineLearningServices/workspaces/julian-ml/PipelineRuns/PipelineSubmit/591b06a0-83c5-4a1e-9798-befc5c0197e8)\n"
     ]
    }
   ],
   "source": [
    "published_pipeline = pipeline.publish(\n",
    "    name=\"my-first-pipeline\",\n",
    "    description=\"Description of my first pipeline.\",\n",
    "    continue_on_step_failure=False,\n",
    ") \n",
    "\n",
    "print(published_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can fetch pipeline using the pipeline ID as follows:\n",
    "# from azureml.pipeline.core import PipelineRun, PublishedPipeline\n",
    "# pipeline = PublishedPipeline.get(workspace=workspace, id=pipeline_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "from azureml.pipeline.core import PipelineRun\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "\n",
    "def _wait_for_run_completion(pipeline_run):\n",
    "    \"\"\"Helper function that waits for the pipeline to finish.\"\"\"\n",
    "    \n",
    "    JOB_STATUS = {\n",
    "        \"not_started\": {0, \"NotStarted\"},\n",
    "        \"running\": {1, \"Running\"},\n",
    "        \"failed\": {2, \"Failed\"},\n",
    "        \"cancelled\": {3, \"Cancelled\"},\n",
    "        \"finished\": {4, \"Finished\"},\n",
    "    }\n",
    "    \n",
    "    print(\"Waiting for job to start...\")\n",
    "    status = pipeline_run.get_status()\n",
    "    while status in JOB_STATUS[\"not_started\"]:\n",
    "        time.sleep(1)\n",
    "        status = pipeline_run.get_status()\n",
    "\n",
    "    if status in JOB_STATUS[\"running\"]:\n",
    "        print(\"Job started, waiting for completion...\")\n",
    "        while status in JOB_STATUS[\"running\"]:\n",
    "            time.sleep(1)\n",
    "            status = pipeline_run.get_status()\n",
    "\n",
    "    if status in JOB_STATUS[\"finished\"]:\n",
    "        print(\"Job finished successfully!\")\n",
    "    elif status in JOB_STATUS[\"failed\"]:\n",
    "        print(\"ERROR: Job failed!\")\n",
    "    elif status in JOB_STATUS[\"cancelled\"]:\n",
    "        print(\"WARNING: Job was cancelled.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected status '{status}'\")\n",
    "        \n",
    "    \n",
    "experiment_name = \"my-first-experiment\"\n",
    "\n",
    "auth = InteractiveLoginAuthentication()\n",
    "aad_token = auth.get_authentication_header()\n",
    "\n",
    "request_payload = {\n",
    "    \"ExperimentName\": experiment_name,\n",
    "    \"ParameterAssignments\": {},\n",
    "}\n",
    "\n",
    "response = requests.post(published_pipeline.endpoint, headers=aad_token, json=request_payload)\n",
    "response.raise_for_status()\n",
    "\n",
    "run_id = response.json()[\"Id\"]\n",
    "print(\"Job ID: %s\" % run_id)\n",
    " \n",
    "pipeline_run = PipelineRun.get(workspace, run_id)\n",
    "_wait_for_run_completion(pipeline_run) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Build the rest of the pipeline.\n",
    "\n",
    "TODO: Screenshot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment: See if you can find the running pipeline in your Azure ML portal. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
