{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying your trained model\n",
    "\n",
    "Once you've managed to run your full ML pipeline, you should end up with a registered versions of your model. You can view the different versions of the model in the Azure ML interface in the *Models* section. Of course, now we have a trained model, we also want to actually deploy our model. \n",
    "\n",
    "There are different approaches for deploying models, including:\n",
    "\n",
    "* Using the GUI (selecting a model > clicking on deploy)\n",
    "* Using the Python SDK\n",
    "\n",
    "In this notebook, we aim to deploy the model using the SDK. (This will also allow us to automate the deployment as part of our CI/CD pipeline later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the model\n",
    "\n",
    "Let's start by fetching the model from our workspace. First, we need to get our workspace again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After which, we can fetch our model using the *Model* class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = \"MY_MODEL_NAME\"\n",
    "\n",
    "# Note: Include version parameter to fetch a specific version. \n",
    "model = Model(workspace=workspace, name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the webservice\n",
    "\n",
    "Now we have our model, we can start deploying it as a webservice. \n",
    "\n",
    "To do so, we first need to define the environment in which to run the model using an instance of the [InferenceConfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Define model environment.\n",
    "model_dir = Path(\"../model\")\n",
    "model_env = Environment.from_conda_specification(f\"{model_name}-env\", model_dir / \"environment.yml\")\n",
    " \n",
    "# Define inference config (= how to serve model).\n",
    "inference_config = InferenceConfig(\n",
    "    entry_script=\"scripts/serve.py\", \n",
    "    source_directory=model_dir, \n",
    "    environment=env\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This *InferenceConfig* is similar to the *RunConfig* used for our pipeline steps, in the sense that this config defines the environment to run in, together with the entrypoint to use for serving the model. Note that the serving entrypoint (*serve.py*) defines how your model responds to requests. This file needs to have a specific structure, on which you can find more details [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py).\n",
    "\n",
    "To actually deploy the webservice, you can use the *Model.deploy* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_name = \"MY_WEBSERVICE_NAME\"\n",
    "\n",
    "# Remove any existing service under the same name.\n",
    "try:\n",
    "    existing_service = Webservice(workspace, name=service_name)\n",
    "    print(\"WARNING: Deleting existing service '%s'\", service_name)\n",
    "    existing_service.delete()\n",
    "except WebserviceException:\n",
    "    pass\n",
    "\n",
    "# Actually deploy the model.\n",
    "print(\"Starting deployment\")\n",
    "service = Model.deploy(workspace, service_name, [model], inference_config)\n",
    "\n",
    "# Wait for deployment to complete.\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After waiting for a moment, this should result in a deployed version of your model with a (publicly) accessible end point>?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Try to query your webservice using the requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load answers/webservice.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
